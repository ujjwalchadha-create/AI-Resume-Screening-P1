!pip install PyPDF2
!pip install python-docx

# âœ… Import necessary libraries
from google.colab import files
import PyPDF2, docx, pandas as pd, numpy as np, re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split

# âœ… Function to extract text
def extract_text(file_path):
    try:
        if file_path.lower().endswith(".pdf"):
            with open(file_path, "rb") as file:
                return "\n".join([page.extract_text() or "" for page in PyPDF2.PdfReader(file).pages]).strip()
        elif file_path.lower().endswith(".docx"):
            return "\n".join([para.text for para in docx.Document(file_path).paragraphs]).strip()
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read().strip()
    except Exception:
        return ""

# âœ… Function to clean text
clean_text = lambda text: re.sub(r'[^\w\s]', ' ', text.lower().strip()) if isinstance(text, str) else ""

# âœ… Upload Job Description
jd_text = clean_text(extract_text(f"/content/{list(files.upload().keys())[0]}"))

# âœ… Upload & Clean Resumes
resume_files = files.upload()
cleaned_resumes = {file: clean_text(extract_text(f"/content/{file}")) for file in resume_files}

# âœ… Load & Process Training Data
data = pd.read_csv("/content/UpdatedResumeDataSet.csv", encoding="utf-8").dropna()
data['cleaned_resume'] = data['Resume'].apply(clean_text)

# âœ… TF-IDF Vectorization
vectorizer = TfidfVectorizer(stop_words="english", max_features=4000)
X, y = vectorizer.fit_transform(data['cleaned_resume']), data['Category']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# âœ… Train Optimized KNN Model
knn = KNeighborsClassifier(n_neighbors=min(5, len(y_train)))
knn.fit(X_train, y_train)

# âœ… Process Resumes & Compute Similarity
resume_tfidf, jd_tfidf = vectorizer.transform(cleaned_resumes.values()), vectorizer.transform([jd_text])
similarity_scores = cosine_similarity(resume_tfidf, jd_tfidf).flatten()

# âœ… Rank & Predict Categories
ranked_resumes = sorted(zip(resume_files.keys(), similarity_scores), key=lambda x: x[1], reverse=True)
predicted_labels = knn.predict(resume_tfidf)

# âœ… Display Results
print("\nðŸ”¹ **Optimized Resume Ranking**\n")
for rank, (file, score) in enumerate(ranked_resumes, 1):
    print(f"{rank}. {file} - Similarity Score: {score:.4f} - Predicted Category: {predicted_labels[rank-1]}")

print(f"\nâœ… Model Accuracy: {knn.score(X_test, y_test) * 100:.2f}%")
print("\nâœ… **Resume Screening & Ranking Completed!**")

import matplotlib.pyplot as plt
import seaborn as sns

# Extract filenames, similarity scores, and categories for visualization
resume_names = [file for file, _ in ranked_resumes]
similarity_scores = [score for _, score in ranked_resumes]

# âœ… Bar Chart: Resume Similarity Scores with Job Description
plt.figure(figsize=(5, 3))
sns.barplot(x=[score for _, score in ranked_resumes],
            y=[file for file, _ in ranked_resumes],
            palette="viridis")  # âœ… Changed color palette to "viridis"
plt.xlabel("Similarity Score")
plt.ylabel("Resumes")
plt.title("Resume Similarity Scores with Job Description")
plt.xlim(0, 1)  # Ensuring the x-axis stays within 0 to 1 range
plt.grid(axis="x", linestyle="--", alpha=0.7)
plt.show()

# âœ… Heatmap: Resume Similarity Scores vs Job Description
plt.figure(figsize=(6, 4))
sns.heatmap(
    [[score] for score in similarity_scores], annot=True, cmap="coolwarm", fmt=".4f",
    xticklabels=["Job Description"], yticklabels=resume_names
)
plt.title("Heatmap of Resume Similarity Scores with JD")
plt.ylabel("Resumes")
plt.show()

